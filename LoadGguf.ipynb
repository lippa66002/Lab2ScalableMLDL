{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook we load the Gguf model that can be run on the cpu"
      ],
      "metadata": {
        "id": "bhpUfqfYEscE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dT0wcgzjWEcr",
        "outputId": "eb65be91-f9bd-4d84-bdf7-1ce381faca8c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting llama-cpp-python\n",
            "  Downloading llama_cpp_python-0.3.16.tar.gz (50.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.7/50.7 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from llama-cpp-python) (4.15.0)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.12/dist-packages (from llama-cpp-python) (2.0.2)\n",
            "Collecting diskcache>=5.6.1 (from llama-cpp-python)\n",
            "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: jinja2>=2.11.3 in /usr/local/lib/python3.12/dist-packages (from llama-cpp-python) (3.1.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2>=2.11.3->llama-cpp-python) (3.0.3)\n",
            "Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: llama-cpp-python\n",
            "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.3.16-cp312-cp312-linux_x86_64.whl size=4422319 sha256=81e805952a6cccfdc0371dbd36f1233b1cb924a269a0a42f8fe988ce80729b6a\n",
            "  Stored in directory: /root/.cache/pip/wheels/90/82/ab/8784ee3fb99ddb07fd36a679ddbe63122cc07718f6c1eb3be8\n",
            "Successfully built llama-cpp-python\n",
            "Installing collected packages: diskcache, llama-cpp-python\n",
            "Successfully installed diskcache-5.6.3 llama-cpp-python-0.3.16\n",
            "\u001b[33m⚠️  Warning: 'huggingface-cli download' is deprecated. Use 'hf download' instead.\u001b[0m\n",
            "Fetching 1 files:   0% 0/1 [00:00<?, ?it/s]Downloading 'llama-3.2-1b-finetome-optimized-q4_k_m.gguf' to 'model/.cache/huggingface/download/nEWReUpR0-Pr9uwnmu5AOWlyPvo=.5c73b60570d313a2c7374c39b8f351926880e4288547ccfdbd87094d415fd217.incomplete'\n",
            "\n",
            "llama-3.2-1b-finetome-optimized-q4_k_m.g(…):   0% 0.00/808M [00:00<?, ?B/s]\u001b[A\n",
            "llama-3.2-1b-finetome-optimized-q4_k_m.g(…):   2% 14.7M/808M [00:01<01:22, 9.67MB/s]\u001b[A\n",
            "llama-3.2-1b-finetome-optimized-q4_k_m.g(…):   2% 16.1M/808M [00:01<01:20, 9.79MB/s]\u001b[A\n",
            "llama-3.2-1b-finetome-optimized-q4_k_m.g(…):   3% 22.1M/808M [00:02<01:36, 8.16MB/s]\u001b[A\n",
            "llama-3.2-1b-finetome-optimized-q4_k_m.g(…):   9% 71.4M/808M [00:03<00:21, 34.0MB/s]\u001b[A\n",
            "llama-3.2-1b-finetome-optimized-q4_k_m.g(…):  17% 138M/808M [00:03<00:08, 80.5MB/s] \u001b[A\n",
            "llama-3.2-1b-finetome-optimized-q4_k_m.g(…):  25% 206M/808M [00:03<00:05, 118MB/s] \u001b[A\n",
            "llama-3.2-1b-finetome-optimized-q4_k_m.g(…):  34% 273M/808M [00:03<00:03, 166MB/s]\u001b[A\n",
            "llama-3.2-1b-finetome-optimized-q4_k_m.g(…):  42% 340M/808M [00:03<00:02, 222MB/s]\u001b[A\n",
            "llama-3.2-1b-finetome-optimized-q4_k_m.g(…):  50% 407M/808M [00:04<00:01, 206MB/s]\u001b[A\n",
            "llama-3.2-1b-finetome-optimized-q4_k_m.g(…):  59% 474M/808M [00:04<00:01, 235MB/s]\u001b[A\n",
            "llama-3.2-1b-finetome-optimized-q4_k_m.g(…):  67% 539M/808M [00:04<00:01, 250MB/s]\u001b[A\n",
            "llama-3.2-1b-finetome-optimized-q4_k_m.g(…):  75% 607M/808M [00:04<00:00, 255MB/s]\u001b[A\n",
            "llama-3.2-1b-finetome-optimized-q4_k_m.g(…):  83% 674M/808M [00:05<00:00, 244MB/s]\u001b[A\n",
            "llama-3.2-1b-finetome-optimized-q4_k_m.g(…):  92% 741M/808M [00:05<00:00, 254MB/s]\u001b[A\n",
            "llama-3.2-1b-finetome-optimized-q4_k_m.g(…): 100% 808M/808M [00:05<00:00, 144MB/s]\n",
            "Download complete. Moving file to model/llama-3.2-1b-finetome-optimized-q4_k_m.gguf\n",
            "Fetching 1 files: 100% 1/1 [00:05<00:00,  5.86s/it]\n",
            "/content/model\n",
            "Downloaded files:\n",
            "  llama-3.2-1b-finetome-optimized-q4_k_m.gguf: 770.3 MB\n",
            "\n",
            "Loading: ./model/llama-3.2-1b-finetome-optimized-q4_k_m.gguf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "/usr/local/lib/python3.12/dist-packages/llama_cpp/llama.py:1242: RuntimeWarning: Detected duplicate leading \"<|begin_of_text|>\" in prompt, this will likely reduce response quality, consider removing it...\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Model loaded!\n",
            "\n",
            "User: What is machine learning?\n",
            "\n",
            "Assistant: Machine learning (ML) is a branch of computer science that uses algorithms to learn and make predictions about data without being explicitly programmed to do so. It is a subfield of artificial intelligence (AI) that involves the use of computers to simulate how the human brain works. The goal of machine learning is to develop algorithms that can automatically learn and improve based on data without being explicitly programmed to do so. This allows for machines to learn from data and make predictions without requiring human intervention. This process is known as machine learning because the machine is doing the learning instead of the programmer. Machine learning is used in a wide range of applications, from image recognition and speech recognition to predictive analytics and natural language processing.\n",
            "\n",
            "The development of machine learning has revolutionized many industries\n"
          ]
        }
      ],
      "source": [
        "# Install llama-cpp-python\n",
        "!pip install llama-cpp-python\n",
        "\n",
        "# Download your GGUF from the Q4_K_M repo\n",
        "!huggingface-cli download lippa6602/llama-3.2-1b-finetome-optimized-Q4_K_M-GGUF \\\n",
        "    --include \"*.gguf\" \\\n",
        "    --local-dir ./model\n",
        "\n",
        "# Check what was downloaded\n",
        "import os\n",
        "print(\"Downloaded files:\")\n",
        "for f in os.listdir(\"./model\"):\n",
        "    if f.endswith('.gguf'):\n",
        "        size = os.path.getsize(f\"./model/{f}\") / (1024**2)\n",
        "        print(f\"  {f}: {size:.1f} MB\")\n",
        "\n",
        "# Load and test\n",
        "from llama_cpp import Llama\n",
        "\n",
        "# Find the GGUF file\n",
        "import glob\n",
        "gguf_file = glob.glob(\"./model/*.gguf\")[0]\n",
        "print(f\"\\nLoading: {gguf_file}\")\n",
        "\n",
        "llm = Llama(\n",
        "    model_path=gguf_file,\n",
        "    n_ctx=2048,\n",
        "    n_threads=2,\n",
        "    n_gpu_layers=0,\n",
        "    verbose=False\n",
        ")\n",
        "\n",
        "print(\"✓ Model loaded!\\n\")\n",
        "\n",
        "# Test generation\n",
        "prompt = \"What is machine learning?\"\n",
        "formatted = f\"\"\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
        "\n",
        "{prompt}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "output = llm(formatted, max_tokens=150, temperature=0.7, stop=[\"<|eot_id|>\"])\n",
        "print(f\"User: {prompt}\")\n",
        "print(f\"\\nAssistant: {output['choices'][0]['text'].strip()}\")"
      ]
    }
  ]
}